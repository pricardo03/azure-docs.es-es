---
title: Tratamiento de datos duplicados
description: En este tema se explican varias maneras de tratar con datos duplicados.
services: data-explorer
author: orspod
ms.author: v-orspod
ms.reviewer: mblythe
ms.service: data-explorer
ms.topic: conceptual
ms.date: 12/19/2018
ms.openlocfilehash: f417ba7d0fcd6f9d6b5bd6cd43cf1730af2ca53c
ms.sourcegitcommit: 25936232821e1e5a88843136044eb71e28911928
ms.translationtype: HT
ms.contentlocale: es-ES
ms.lasthandoff: 01/04/2019
ms.locfileid: "54027962"
---
# <a name="deal-with-duplicate-data"></a>Tratamiento de datos duplicados

Los dispositivos que envían datos a la nube mantienen una caché local de los datos. Según el tamaño de los datos, la memoria caché local puede almacenar datos de varios días o incluso meses. Es conveniente proteger las bases de datos analíticos de aquellos dispositivos que por error vuelven a enviar los datos almacenados en caché y provocan la duplicación de datos en la base de datos analíticos. En este tema se describen los procedimientos recomendados para controlar los datos duplicados para estos tipos de escenarios.

La mejor solución para la duplicación de datos es evitar dicha duplicación. Si es posible, corrija el problema con antelación en la canalización de datos, lo que ahorrará costos asociados con el movimiento de datos a lo largo de la canalización de datos y evitará que se empleen recursos en tratar con los datos duplicados introducidos en el sistema. Sin embargo, en situaciones en las que no es posible modificar el sistema de origen, hay varias maneras de gestionar este escenario.

## <a name="understand-the-impact-of-duplicate-data"></a>Conocimiento del impacto de los datos duplicados

Supervise el porcentaje de datos duplicados. Una vez detectado el porcentaje de datos duplicados, puede analizar el ámbito del problema y el impacto sobre el negocio para elegir la solución adecuada.

Ejemplo de consulta para identificar el porcentaje de registros duplicados:

```kusto
let _sample = 0.01; // 1% sampling
let _data =
DeviceEventsAll
| where EventDateTime between (datetime('10-01-2018 10:00') .. datetime('10-10-2018 10:00'));
let _totalRecords = toscalar(_data | count);
_data
| where rand()<= _sample
| summarize recordsCount=count() by hash(DeviceId) + hash(EventId) + hash(StationId)  // Use all dimensions that make row unique. Combining hashes can be improved
| summarize duplicateRecords=countif(recordsCount  > 1)
| extend duplicate_percentage = (duplicateRecords / _sample) / _totalRecords  
```

## <a name="solutions-for-handling-duplicate-data"></a>Soluciones para tratar datos duplicados

### <a name="solution-1-dont-remove-duplicate-data"></a>Solución 1: No eliminar los datos duplicados

Determine los requisitos de su negocio y su nivel de tolerancia a los datos duplicados. Algunos conjuntos de datos pueden aceptar un determinado porcentaje de datos duplicados. Si los datos duplicados no tienen una repercusión importante, puede ignorar su presencia. La ventaja de no eliminar los datos duplicados es que no se sobrecargará adicionalmente el proceso de ingesta o el rendimiento de consultas.

### <a name="solution-2-handle-duplicate-rows-during-query"></a>Solución 2: Administrar las filas duplicadas durante la consulta

Otra opción es filtrar las filas duplicadas en los datos durante la consulta. La función de agregado [`arg_max()`](/azure/kusto/query/arg-max-aggfunction) se puede usar para filtrar los registros duplicados y devolver el último registro en función de la marca de tiempo (u otra columna). La ventaja de este método es una ingesta más rápida, ya que la desduplicación se produce durante el tiempo de consulta. Además, todos los registros (incluidos los duplicados) están disponibles para auditoría y solución de problemas. La desventaja de utilizar la función `arg_max` es el tiempo de consulta adicional y la carga en la CPU cada vez que se consultan los datos. Según la cantidad de datos consultados, esta solución podría no ser funcional o consumir demasiada memoria, por lo que sería necesario recurrir a otras opciones.

En el ejemplo siguiente, se consulta el último registro ingerido para un conjunto de columnas que determinan los registros únicos:

```kusto
DeviceEventsAll
| where EventDateTime > ago(90d)
| summarize hint.strategy=shuffle arg_max(EventDateTime, *) by DeviceId, EventId, StationId
```

Esta consulta también se puede colocar dentro de una función en lugar de consultar directamente la tabla:

```kusto
.create function DeviceEventsView
{
DeviceEventsAll
| where EventDateTime > ago(90d)
| summarize arg_max(EventDateTime, *) by DeviceId, EventId, StationId
}
```

### <a name="solution-3-filter-duplicates-during-the-ingestion-process"></a>Solución 3: Filtrar duplicados durante el proceso de ingesta

Otra solución consiste en filtrar duplicados durante el proceso de ingesta. El sistema ignora los datos duplicados durante la ingesta en tablas Kusto. Los datos se ingieren en una tabla de almacenamiento provisional y se copian en otra tabla después de eliminar las filas duplicadas. La ventaja de esta solución es que el rendimiento de las consultas mejora drásticamente en comparación con la solución anterior. Entre sus desventajas se incluyen un mayor tiempo de ingesta y costos adicionales de almacenamiento de datos.

El siguiente ejemplo describe este método:

1. Cree otra tabla con el mismo esquema:

    ```kusto
    .create table DeviceEventsUnique (EventDateTime: datetime, DeviceId: int, EventId: int, StationId: int)
    ```

1. Cree una función para filtrar los registros duplicados mediante la anticombinación de los nuevos registros con los ingeridos anteriormente.

    ```kusto
    .create function RemoveDuplicateDeviceEvents()
    {
    DeviceEventsAll
    | join hint.strategy=broadcast kind = anti
        (
        DeviceEventsUnique
        | where EventDateTime > ago(7d)   // filter the data for certain time frame
        | limit 1000000   //set some limitations (few million records) to avoid choking-up the system during outage recovery

        ) on DeviceId, EventId, StationId
    }
    ```

    > [!NOTE]
    > Las combinaciones son operaciones vinculadas a la CPU y agregan una carga adicional al sistema.

1. Establezca la [directiva de actualización](/azure/kusto/management/update-policy) en la tabla `DeviceEventsUnique`. La directiva de actualización se activa cuando entran nuevos datos en la tabla `DeviceEventsAll`. El motor de Kusto ejecutará automáticamente la función a medida que se crean nuevas [extensiones](/azure/kusto/management/extents-overview). El procesamiento se limita a los datos recién creados. El comando siguiente une la tabla de origen (`DeviceEventsAll`), la tabla de destino (`DeviceEventsUnique`) y la función `RemoveDuplicatesDeviceEvents` para crear la directiva de actualización.

    ```kusto
    .alter table DeviceEventsUnique policy update
    @'[{"IsEnabled": true, "Source": "DeviceEventsAll", "Query": "RemoveDuplicateDeviceEvents()", "IsTransactional": true, "PropagateIngestionProperties": true}]'
    ```

    > [!NOTE]
    > La directiva de actualización prolonga la duración de la ingesta, puesto que los datos se filtran durante la ingesta y, a continuación, son ingeridos dos veces (para la tabla `DeviceEventsAll` y para la tabla `DeviceEventsUnique`).

1. (Opcional) Defina una retención de datos inferior en la tabla `DeviceEventsAll` para evitar almacenar copias de los datos. Seleccione el número de días según el volumen de datos y el período de tiempo que desee retener los datos para la solución de problemas. Puede definir la retención en `0d` días para ahorrar costos de bienes vendidos (COGS) y mejorar el rendimiento, ya que los datos no se cargan en el almacenamiento.

    ```kusto
    .alter-merge table DeviceEventsAll policy retention softdelete = 1d
    ```

## <a name="summary"></a>Resumen

La duplicación de datos se puede tratar de varias maneras. Evalúe las opciones detenidamente, teniendo en cuenta el precio y el rendimiento, para determinar el método correcto para su negocio.

## <a name="next-steps"></a>Pasos siguientes

> [!div class="nextstepaction"]
> [Write queries for Azure Data Explorer](write-queries.md) (Escritura de consultas del Explorador de datos de Azure)
